"""
API Server cho AI Exam Generator - TÃ­ch há»£p vá»›i Backend
Cháº¡y riÃªng biá»‡t vá»›i backend, expose REST API
"""
from flask import Flask, request, jsonify
from flask_cors import CORS
import os
import sys
from typing import List, Dict
from loguru import logger
import requests
from sentence_transformers import SentenceTransformer
import numpy as np
import faiss

from src.models import Chunk, GlobalConfig, CognitiveRatios, DifficultyRatios
from exam_pipeline import ExamPipeline

# Setup Flask
app = Flask(__name__)
CORS(app)  # Enable CORS cho frontend

# Global variables
embedder = None
rag_index = None
chunks_store = []
pipeline = None
ollama_base_url = "http://localhost:11434"
ollama_model = "qwen2.5:3b"


class LocalEmbeddings:
    """Embeddings sá»­ dá»¥ng sentence-transformers (local)"""
    
    def __init__(self, model_name: str = "paraphrase-multilingual-MiniLM-L12-v2"):
        logger.info(f"ðŸ”§ Loading embedding model: {model_name}")
        self.model = SentenceTransformer(model_name)
        logger.info(f"âœ… Model loaded")
    
    def encode(self, texts: List[str]) -> np.ndarray:
        """Táº¡o embeddings cho danh sÃ¡ch text"""
        return self.model.encode(texts, show_progress_bar=False)


def init_models():
    """Initialize AI models"""
    global embedder, rag_index, chunks_store, pipeline
    
    logger.info("ðŸš€ Initializing AI models...")
    embedder = LocalEmbeddings()
    
    # Initialize pipeline
    pipeline = ExamPipeline(
        embedder=embedder.model,
        ollama_url=ollama_base_url,
        ollama_model=ollama_model
    )
    
    logger.info("âœ… Models initialized")


def generate_with_ollama(prompt: str, system: str = "") -> str:
    """Generate text tá»« Ollama"""
    url = f"{ollama_base_url}/api/generate"
    
    payload = {
        "model": ollama_model,
        "prompt": prompt,
        "system": system,
        "stream": False
    }
    
    try:
        response = requests.post(url, json=payload, timeout=120)
        response.raise_for_status()
        return response.json()['response']
    except Exception as e:
        logger.error(f"âŒ Ollama error: {e}")
        return ""


def chunk_text(text: str, chunk_size: int = 500) -> List[Chunk]:
    """Chia vÄƒn báº£n thÃ nh chunks"""
    chunks = []
    lines = text.strip().split('\n')
    current_chunk = ""
    current_section = ""
    chunk_id = 0
    char_pos = 0
    
    for line in lines:
        if line.startswith("CHÆ¯Æ NG") or line.startswith("Chapter"):
            current_section = line
        
        if len(current_chunk) + len(line) > chunk_size and current_chunk:
            chunk_text_val = current_chunk.strip()
            chunks.append(Chunk(
                chunk_id=f"c{chunk_id:03d}",
                page=1,
                section=current_section,
                text=chunk_text_val,
                char_start=char_pos,
                char_end=char_pos + len(chunk_text_val)
            ))
            char_pos += len(chunk_text_val)
            chunk_id += 1
            current_chunk = line + "\n"
        else:
            current_chunk += line + "\n"
    
    if current_chunk.strip():
        chunk_text_val = current_chunk.strip()
        chunks.append(Chunk(
            chunk_id=f"c{chunk_id:03d}",
            page=1,
            section=current_section,
            text=chunk_text_val,
            char_start=char_pos,
            char_end=char_pos + len(chunk_text_val)
        ))
    
    logger.info(f"âœ… Created {len(chunks)} chunks")
    return chunks


def build_rag_index(chunks: List[Chunk]):
    """Build FAISS index tá»« chunks"""
    global rag_index, chunks_store
    
    logger.info(f"ðŸ” Building RAG index for {len(chunks)} chunks...")
    chunks_store = chunks
    
    # Táº¡o embeddings
    texts = [c.text for c in chunks]
    embeddings = embedder.encode(texts)
    
    # Build FAISS index
    dimension = embeddings.shape[1]
    rag_index = faiss.IndexFlatL2(dimension)
    rag_index.add(embeddings.astype('float32'))
    
    logger.info(f"âœ… RAG index built with {rag_index.ntotal} vectors")


def search_rag(query: str, top_k: int = 3) -> List[Dict]:
    """TÃ¬m kiáº¿m chunks liÃªn quan"""
    if rag_index is None or not chunks_store:
        return []
    
    # Embed query
    query_embedding = embedder.encode([query])
    
    # Search
    distances, indices = rag_index.search(query_embedding.astype('float32'), top_k)
    
    results = []
    for idx, dist in zip(indices[0], distances[0]):
        if 0 <= idx < len(chunks_store):
            chunk = chunks_store[idx]
            results.append({
                "chunk_id": chunk.chunk_id,
                "text": chunk.text,
                "section": chunk.section,
                "distance": float(dist)
            })
    
    return results


# ==================== API ENDPOINTS ====================

@app.route('/', methods=['GET'])
def index():
    """API Documentation - Root endpoint"""
    return jsonify({
        "name": "ðŸŽ“ AI Exam Generator API",
        "version": "1.0.0",
        "description": "Generate exam questions from PDF documents using 100% local AI (Ollama + sentence-transformers)",
        "status": "running",
        "endpoints": {
            "GET /health": "Health check",
            "POST /upload-document": "Upload and chunk document",
            "POST /generate-blueprint": "Extract blueprint from document",
            "POST /generate-mcq": "Generate MCQ questions",
            "POST /generate-essay": "Generate essay questions",
            "POST /search": "RAG search in document",
            "POST /generate-exam-from-pdf": "ðŸš€ FULL PIPELINE - Generate complete exam from PDF"
        },
        "test_ui": "http://localhost:8080/test_ui.html",
        "server_port": os.getenv('AI_PORT', '5001'),
        "ai_models": {
            "embedder": "paraphrase-multilingual-MiniLM-L12-v2",
            "llm": f"{ollama_model} (Ollama)",
            "ollama_url": ollama_base_url
        },
        "usage": {
            "curl_example": f"curl http://localhost:{os.getenv('AI_PORT', '5001')}/health"
        }
    })


@app.route('/health', methods=['GET'])
def health_check():
    """Health check endpoint"""
    return jsonify({
        "status": "healthy",
        "embedder_loaded": embedder is not None,
        "rag_index_loaded": rag_index is not None,
        "chunks_count": len(chunks_store),
        "ollama_url": ollama_base_url,
        "ollama_model": ollama_model
    })


@app.route('/upload-document', methods=['POST'])
def upload_document():
    """Upload vÃ  xá»­ lÃ½ tÃ i liá»‡u"""
    try:
        data = request.get_json()
        content = data.get('content', '')
        
        if not content:
            return jsonify({"error": "Content is required"}), 400
        
        # Chunking
        chunks = chunk_text(content)
        
        # Build RAG index
        build_rag_index(chunks)
        
        return jsonify({
            "success": True,
            "chunks_count": len(chunks),
            "message": "Document processed successfully"
        })
        
    except Exception as e:
        logger.error(f"Error in upload_document: {e}")
        return jsonify({"error": str(e)}), 500


@app.route('/generate-blueprint', methods=['POST'])
def generate_blueprint():
    """Sinh ma tráº­n Ä‘á» thi"""
    try:
        data = request.get_json()
        config = data.get('config', {})
        
        # Search context
        context_chunks = search_rag("ma tráº­n Ä‘á» kiá»ƒm tra", top_k=3)
        context = "\n\n".join([c['text'] for c in context_chunks])
        
        time_minutes = config.get('time_minutes', 45)
        total_points = config.get('total_points', 10.0)
        mcq_ratio = config.get('mcq_ratio', 0.6)
        essay_ratio = config.get('essay_ratio', 0.4)
        
        prompt = f"""Dá»±a vÃ o tÃ i liá»‡u sau, hÃ£y táº¡o ma tráº­n Ä‘á» kiá»ƒm tra:

TÃ€I LIá»†U:
{context}

YÃŠU Cáº¦U:
- Thá»i gian: {time_minutes} phÃºt
- Tá»•ng Ä‘iá»ƒm: {total_points}
- Tá»‰ lá»‡ tráº¯c nghiá»‡m: {mcq_ratio*100}%
- Tá»‰ lá»‡ tá»± luáº­n: {essay_ratio*100}%

HÃ£y táº¡o báº£ng ma tráº­n chi tiáº¿t vá»›i cÃ¡c cá»™t:
- Ná»™i dung kiáº¿n thá»©c
- Má»©c Ä‘á»™ (Biáº¿t, Hiá»ƒu, Váº­n dá»¥ng, Váº­n dá»¥ng cao)
- Sá»‘ cÃ¢u
- Sá»‘ Ä‘iá»ƒm

Chá»‰ tráº£ vá» báº£ng ma tráº­n, khÃ´ng giáº£i thÃ­ch."""

        blueprint = generate_with_ollama(prompt, system="Báº¡n lÃ  chuyÃªn gia giÃ¡o dá»¥c, táº¡o ma tráº­n Ä‘á» thi chÃ­nh xÃ¡c.")
        
        return jsonify({
            "success": True,
            "blueprint": blueprint,
            "context_used": len(context_chunks)
        })
        
    except Exception as e:
        logger.error(f"Error in generate_blueprint: {e}")
        return jsonify({"error": str(e)}), 500


@app.route('/generate-mcq', methods=['POST'])
def generate_mcq():
    """Sinh cÃ¢u há»i tráº¯c nghiá»‡m"""
    try:
        data = request.get_json()
        topic = data.get('topic', '')
        num_questions = data.get('num_questions', 3)
        
        # Search context
        context_chunks = search_rag(topic, top_k=2)
        context = "\n\n".join([c['text'] for c in context_chunks])
        
        prompt = f"""Dá»±a vÃ o tÃ i liá»‡u:

{context}

HÃ£y táº¡o {num_questions} cÃ¢u há»i tráº¯c nghiá»‡m vá» {topic}.

Má»—i cÃ¢u gá»“m:
- Äá» bÃ i
- 4 Ä‘Ã¡p Ã¡n A, B, C, D
- ÄÃ¡p Ã¡n Ä‘Ãºng
- Giáº£i thÃ­ch ngáº¯n

Format:
CÃ¢u 1: [Ä‘á» bÃ i]
A. [Ä‘Ã¡p Ã¡n A]
B. [Ä‘Ã¡p Ã¡n B]
C. [Ä‘Ã¡p Ã¡n C]
D. [Ä‘Ã¡p Ã¡n D]
ÄÃ¡p Ã¡n: [A/B/C/D]
Giáº£i thÃ­ch: [lÃ½ do]
"""
        
        mcq = generate_with_ollama(prompt, system="Táº¡o cÃ¢u há»i tráº¯c nghiá»‡m cháº¥t lÆ°á»£ng cao.")
        
        return jsonify({
            "success": True,
            "questions": mcq,
            "context_used": len(context_chunks)
        })
        
    except Exception as e:
        logger.error(f"Error in generate_mcq: {e}")
        return jsonify({"error": str(e)}), 500


@app.route('/generate-essay', methods=['POST'])
def generate_essay():
    """Sinh cÃ¢u há»i tá»± luáº­n"""
    try:
        data = request.get_json()
        topic = data.get('topic', '')
        
        # Search context
        context_chunks = search_rag(topic, top_k=2)
        context = "\n\n".join([c['text'] for c in context_chunks])
        
        prompt = f"""Dá»±a vÃ o tÃ i liá»‡u:

{context}

HÃ£y táº¡o 1 bÃ i toÃ¡n thá»±c táº¿ vá» {topic}.

YÃªu cáº§u:
- Äá» bÃ i gáº¯n liá»n thá»±c táº¿
- HÆ°á»›ng dáº«n giáº£i chi tiáº¿t
- ÄÃ¡p sá»‘ rÃµ rÃ ng

Format:
BÃ i toÃ¡n: [Ä‘á» bÃ i]
HÆ°á»›ng dáº«n giáº£i:
[cÃ¡c bÆ°á»›c giáº£i]
ÄÃ¡p sá»‘: [káº¿t quáº£]
"""
        
        essay = generate_with_ollama(prompt, system="Táº¡o bÃ i toÃ¡n thá»±c táº¿ hay vÃ  phÃ¹ há»£p.")
        
        return jsonify({
            "success": True,
            "question": essay,
            "context_used": len(context_chunks)
        })
        
    except Exception as e:
        logger.error(f"Error in generate_essay: {e}")
        return jsonify({"error": str(e)}), 500


@app.route('/search', methods=['POST'])
def search():
    """TÃ¬m kiáº¿m trong tÃ i liá»‡u"""
    try:
        data = request.get_json()
        query = data.get('query', '')
        top_k = data.get('top_k', 5)
        
        results = search_rag(query, top_k=top_k)
        
        return jsonify({
            "success": True,
            "results": results
        })
        
    except Exception as e:
        logger.error(f"Error in search: {e}")
        return jsonify({"error": str(e)}), 500


@app.route('/generate-exam-from-pdf', methods=['POST'])
def generate_exam_from_pdf():
    """
    PRODUCTION ENDPOINT: Complete pipeline
    PDF content â†’ Blueprint â†’ Matrix â†’ Questions â†’ Validation â†’ Export
    
    Input:
        {
            "content": "PDF text content",
            "config": {
                "time_minutes": 45,
                "total_points": 10.0,
                "mcq_ratio": 0.6,
                "essay_ratio": 0.4
            },
            "cognitive_ratios": {
                "biet": 0.3,
                "hieu": 0.3,
                "vandung": 0.3,
                "vandungcao": 0.1
            },
            "difficulty_ratios": {
                "de": 0.3,
                "tb": 0.4,
                "kho": 0.3
            }
        }
    
    Output:
        {
            "success": true,
            "blueprint": {...},
            "matrix": {...},
            "exam": {...},
            "validation": {
                "valid": true/false,
                "issues": [...]
            },
            "stats": {...}
        }
    """
    try:
        data = request.get_json()
        
        content = data.get('content', '')
        if not content:
            return jsonify({"error": "Content is required"}), 400
        
        # Parse config
        config_data = data.get('config', {})
        config = GlobalConfig(
            time_minutes=config_data.get('time_minutes', 45),
            total_points=config_data.get('total_points', 10.0),
            mcq_ratio=config_data.get('mcq_ratio', 0.6),
            essay_ratio=config_data.get('essay_ratio', 0.4)
        )
        
        cognitive_data = data.get('cognitive_ratios', {})
        cognitive_ratios = CognitiveRatios(
            biet=cognitive_data.get('biet', 0.3),
            hieu=cognitive_data.get('hieu', 0.3),
            vandung=cognitive_data.get('vandung', 0.3),
            vandungcao=cognitive_data.get('vandungcao', 0.1)
        )
        
        difficulty_data = data.get('difficulty_ratios', {})
        difficulty_ratios = DifficultyRatios(
            de=difficulty_data.get('de', 0.3),
            tb=difficulty_data.get('tb', 0.4),
            kho=difficulty_data.get('kho', 0.3)
        )
        
        # Run pipeline
        logger.info("ðŸš€ Running full exam generation pipeline...")
        result = pipeline.run_full_pipeline(
            pdf_content=content,
            config=config,
            cognitive_ratios=cognitive_ratios,
            difficulty_ratios=difficulty_ratios,
            output_dir="./output"
        )
        
        # Return results
        return jsonify({
            "success": True,
            "blueprint": {
                "topics": [t.model_dump() for t in result['blueprint'].topics],
                "outcomes": [o.model_dump() for o in result['blueprint'].outcomes]
            },
            "matrix": {
                "total_items": len(result['matrix'].items),
                "total_points": sum(item.points_each * item.num_questions for item in result['matrix'].items)
            },
            "exam": {
                "title": result['exam'].title,
                "total_questions": len(result['exam'].questions),
                "total_points": result['exam'].total_points,
                "questions": [q.model_dump() for q in result['exam'].questions]
            },
            "validation": {
                "valid": result['valid']
            },
            "message": "Exam generated successfully! Files saved to ./output/"
        })
        
    except Exception as e:
        logger.error(f"Error in generate_exam_from_pdf: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({"error": str(e)}), 500


if __name__ == '__main__':
    # Initialize models
    init_models()
    
    # Start server
    port = int(os.environ.get('AI_PORT', 5001))
    logger.info(f"ðŸš€ Starting AI API Server on port {port}")
    app.run(host='0.0.0.0', port=port, debug=False)
