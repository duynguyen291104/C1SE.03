"""
Demo s·ª≠ d·ª•ng AI Local - KH√îNG C·∫¶N OPENAI
S·ª≠ d·ª•ng Ollama cho LLM v√† sentence-transformers cho embeddings
"""
import os
import sys
import json
from pathlib import Path
from typing import List, Dict
from loguru import logger
import requests

# Sentence transformers cho embeddings
from sentence_transformers import SentenceTransformer
import numpy as np
import faiss

from src.models import (
    Document, DocumentPage, DocumentMetadata, Chunk, 
    GlobalConfig, CognitiveRatios, DifficultyRatios
)


def create_sample_content() -> str:
    """T·∫°o n·ªôi dung m·∫´u v·ªÅ To√°n 9"""
    logger.info("üìÑ T·∫°o document m·∫´u...")
    return """
CH∆Ø∆†NG 1: H·ªÜ PH∆Ø∆†NG TR√åNH B·∫¨C NH·∫§T HAI ·∫®N

I. M·ª§C TI√äU
- H·ªçc sinh bi·∫øt kh√°i ni·ªám ph∆∞∆°ng tr√¨nh b·∫≠c nh·∫•t hai ·∫©n, h·ªá ph∆∞∆°ng tr√¨nh b·∫≠c nh·∫•t hai ·∫©n
- H·ªçc sinh bi·∫øt c√°ch gi·∫£i h·ªá ph∆∞∆°ng tr√¨nh b·∫±ng ph∆∞∆°ng ph√°p th·∫ø v√† ph∆∞∆°ng ph√°p c·ªông ƒë·∫°i s·ªë
- V·∫≠n d·ª•ng gi·∫£i b√†i to√°n th·ª±c t·∫ø

II. N·ªòI DUNG

1. Ph∆∞∆°ng tr√¨nh b·∫≠c nh·∫•t hai ·∫©n
- D·∫°ng t·ªïng qu√°t: ax + by = c
- Nghi·ªám c·ªßa ph∆∞∆°ng tr√¨nh
- T·∫≠p nghi·ªám

2. H·ªá ph∆∞∆°ng tr√¨nh b·∫≠c nh·∫•t hai ·∫©n
- H·ªá c√≥ nghi·ªám duy nh·∫•t
- H·ªá v√¥ nghi·ªám
- H·ªá v√¥ s·ªë nghi·ªám

3. Ph∆∞∆°ng ph√°p gi·∫£i
- Ph∆∞∆°ng ph√°p th·∫ø
- Ph∆∞∆°ng ph√°p c·ªông ƒë·∫°i s·ªë
- Ph∆∞∆°ng ph√°p ƒë·ªì th·ªã

CH∆Ø∆†NG 2: H√ÄM S·ªê B·∫¨C NH·∫§T

I. Kh√°i ni·ªám
- H√†m s·ªë y = ax + b (a ‚â† 0)
- T√≠nh ch·∫•t ƒë·ªìng bi·∫øn, ngh·ªãch bi·∫øn

II. ƒê·ªì th·ªã h√†m s·ªë b·∫≠c nh·∫•t
- ƒê·ªì th·ªã l√† ƒë∆∞·ªùng th·∫≥ng
- V·∫Ω ƒë·ªì th·ªã
- V·ªã tr√≠ t∆∞∆°ng ƒë·ªëi c·ªßa hai ƒë∆∞·ªùng th·∫≥ng
"""


def chunk_text(text: str, chunk_size: int = 500) -> List[Chunk]:
    """Chia vƒÉn b·∫£n th√†nh chunks"""
    chunks = []
    lines = text.strip().split('\n')
    current_chunk = ""
    current_section = ""
    chunk_id = 0
    char_pos = 0
    
    for line in lines:
        if line.startswith("CH∆Ø∆†NG"):
            current_section = line
        
        if len(current_chunk) + len(line) > chunk_size and current_chunk:
            chunk_text_val = current_chunk.strip()
            chunks.append(Chunk(
                chunk_id=f"c{chunk_id:03d}",
                page=1,
                section=current_section,
                text=chunk_text_val,
                char_start=char_pos,
                char_end=char_pos + len(chunk_text_val)
            ))
            char_pos += len(chunk_text_val)
            chunk_id += 1
            current_chunk = line + "\n"
        else:
            current_chunk += line + "\n"
    
    if current_chunk.strip():
        chunk_text_val = current_chunk.strip()
        chunks.append(Chunk(
            chunk_id=f"c{chunk_id:03d}",
            page=1,
            section=current_section,
            text=chunk_text_val,
            char_start=char_pos,
            char_end=char_pos + len(chunk_text_val)
        ))
    
    logger.info(f"‚úÖ Created {len(chunks)} chunks")
    return chunks


class LocalEmbeddings:
    """Embeddings s·ª≠ d·ª•ng sentence-transformers (local)"""
    
    def __init__(self, model_name: str = "paraphrase-multilingual-MiniLM-L12-v2"):
        logger.info(f"üîß Loading embedding model: {model_name}")
        self.model = SentenceTransformer(model_name)
        logger.info(f"‚úÖ Model loaded")
    
    def encode(self, texts: List[str]) -> np.ndarray:
        """T·∫°o embeddings cho danh s√°ch text"""
        return self.model.encode(texts, show_progress_bar=True)


class LocalRAG:
    """RAG s·ª≠ d·ª•ng FAISS v√† local embeddings"""
    
    def __init__(self):
        self.embedder = LocalEmbeddings()
        self.index = None
        self.chunks = []
    
    def build_index(self, chunks: List[Chunk]):
        """Build FAISS index t·ª´ chunks"""
        logger.info(f"üîç Building RAG index for {len(chunks)} chunks...")
        self.chunks = chunks
        
        # T·∫°o embeddings
        texts = [c.text for c in chunks]
        embeddings = self.embedder.encode(texts)
        
        # Build FAISS index
        dimension = embeddings.shape[1]
        self.index = faiss.IndexFlatL2(dimension)
        self.index.add(embeddings.astype('float32'))
        
        logger.info(f"‚úÖ RAG index built with {self.index.ntotal} vectors")
    
    def search(self, query: str, top_k: int = 3) -> List[Chunk]:
        """T√¨m ki·∫øm chunks li√™n quan"""
        if self.index is None:
            return []
        
        # Embed query
        query_embedding = self.embedder.encode([query])
        
        # Search
        distances, indices = self.index.search(query_embedding.astype('float32'), top_k)
        
        results = []
        for idx in indices[0]:
            if 0 <= idx < len(self.chunks):
                results.append(self.chunks[idx])
        
        return results


class OllamaLLM:
    """LLM s·ª≠ d·ª•ng Ollama (local)"""
    
    def __init__(self, model: str = "qwen2.5:3b", base_url: str = "http://localhost:11434"):
        self.model = model
        self.base_url = base_url
        logger.info(f"ü§ñ Using Ollama model: {model}")
    
    def generate(self, prompt: str, system: str = "") -> str:
        """Generate text t·ª´ prompt"""
        url = f"{self.base_url}/api/generate"
        
        payload = {
            "model": self.model,
            "prompt": prompt,
            "system": system,
            "stream": False
        }
        
        try:
            response = requests.post(url, json=payload, timeout=120)
            response.raise_for_status()
            return response.json()['response']
        except Exception as e:
            logger.error(f"‚ùå Ollama error: {e}")
            return ""


def generate_exam_local(rag: LocalRAG, llm: OllamaLLM, config: GlobalConfig):
    """Sinh ƒë·ªÅ ki·ªÉm tra s·ª≠ d·ª•ng AI local"""
    
    logger.info("\n" + "="*80)
    logger.info("üéØ B∆Ø·ªöC 1: SINH MA TR·∫¨N ƒê·ªÄ")
    logger.info("="*80)
    
    # Search context
    context_chunks = rag.search("ma tr·∫≠n ƒë·ªÅ ki·ªÉm tra to√°n 9 h·ªá ph∆∞∆°ng tr√¨nh", top_k=3)
    context = "\n\n".join([c.text for c in context_chunks])
    
    matrix_prompt = f"""D·ª±a v√†o t√†i li·ªáu sau, h√£y t·∫°o ma tr·∫≠n ƒë·ªÅ ki·ªÉm tra To√°n 9:

T√ÄI LI·ªÜU:
{context}

Y√äU C·∫¶U:
- Th·ªùi gian: {config.time_minutes} ph√∫t
- T·ªïng ƒëi·ªÉm: {config.total_points}
- T·ªâ l·ªá tr·∫Øc nghi·ªám: {config.mcq_ratio*100}%
- T·ªâ l·ªá t·ª± lu·∫≠n: {config.essay_ratio*100}%

H√£y t·∫°o b·∫£ng ma tr·∫≠n chi ti·∫øt v·ªõi c√°c c·ªôt:
- N·ªôi dung ki·∫øn th·ª©c
- M·ª©c ƒë·ªô (Bi·∫øt, Hi·ªÉu, V·∫≠n d·ª•ng, V·∫≠n d·ª•ng cao)
- S·ªë c√¢u
- S·ªë ƒëi·ªÉm

Ch·ªâ tr·∫£ v·ªÅ b·∫£ng ma tr·∫≠n, kh√¥ng gi·∫£i th√≠ch."""

    matrix = llm.generate(matrix_prompt, system="B·∫°n l√† chuy√™n gia gi√°o d·ª•c, t·∫°o ma tr·∫≠n ƒë·ªÅ thi ch√≠nh x√°c.")
    logger.info(f"\n{matrix}\n")
    
    logger.info("\n" + "="*80)
    logger.info("üìù B∆Ø·ªöC 2: SINH C√ÇU H·ªéI TR·∫ÆC NGHI·ªÜM")
    logger.info("="*80)
    
    mcq_context = rag.search("h·ªá ph∆∞∆°ng tr√¨nh b·∫≠c nh·∫•t ph∆∞∆°ng ph√°p th·∫ø", top_k=2)
    mcq_ctx = "\n\n".join([c.text for c in mcq_context])
    
    mcq_prompt = f"""D·ª±a v√†o t√†i li·ªáu:

{mcq_ctx}

H√£y t·∫°o 3 c√¢u h·ªèi tr·∫Øc nghi·ªám v·ªÅ h·ªá ph∆∞∆°ng tr√¨nh b·∫≠c nh·∫•t hai ·∫©n.

M·ªói c√¢u g·ªìm:
- ƒê·ªÅ b√†i
- 4 ƒë√°p √°n A, B, C, D
- ƒê√°p √°n ƒë√∫ng
- Gi·∫£i th√≠ch ng·∫Øn

Format:
C√¢u 1: [ƒë·ªÅ b√†i]
A. [ƒë√°p √°n A]
B. [ƒë√°p √°n B]
C. [ƒë√°p √°n C]
D. [ƒë√°p √°n D]
ƒê√°p √°n: [A/B/C/D]
Gi·∫£i th√≠ch: [l√Ω do]
"""
    
    mcq_questions = llm.generate(mcq_prompt, system="T·∫°o c√¢u h·ªèi tr·∫Øc nghi·ªám ch·∫•t l∆∞·ª£ng cao.")
    logger.info(f"\n{mcq_questions}\n")
    
    logger.info("\n" + "="*80)
    logger.info("‚úçÔ∏è B∆Ø·ªöC 3: SINH C√ÇU H·ªéI T·ª∞ LU·∫¨N")
    logger.info("="*80)
    
    essay_context = rag.search("v·∫≠n d·ª•ng gi·∫£i b√†i to√°n th·ª±c t·∫ø h·ªá ph∆∞∆°ng tr√¨nh", top_k=2)
    essay_ctx = "\n\n".join([c.text for c in essay_context])
    
    essay_prompt = f"""D·ª±a v√†o t√†i li·ªáu:

{essay_ctx}

H√£y t·∫°o 1 b√†i to√°n th·ª±c t·∫ø v·ªÅ h·ªá ph∆∞∆°ng tr√¨nh b·∫≠c nh·∫•t hai ·∫©n.

Y√™u c·∫ßu:
- ƒê·ªÅ b√†i g·∫Øn li·ªÅn th·ª±c t·∫ø
- H∆∞·ªõng d·∫´n gi·∫£i chi ti·∫øt
- ƒê√°p s·ªë r√µ r√†ng

Format:
B√†i to√°n: [ƒë·ªÅ b√†i]
H∆∞·ªõng d·∫´n gi·∫£i:
[c√°c b∆∞·ªõc gi·∫£i]
ƒê√°p s·ªë: [k·∫øt qu·∫£]
"""
    
    essay_question = llm.generate(essay_prompt, system="T·∫°o b√†i to√°n th·ª±c t·∫ø hay v√† ph√π h·ª£p.")
    logger.info(f"\n{essay_question}\n")
    
    logger.info("\n" + "="*80)
    logger.info("‚úÖ HO√ÄN TH√ÄNH!")
    logger.info("="*80)


def main():
    """Main function"""
    try:
        logger.info("="*80)
        logger.info("üéØ DEMO H·ªÜ TH·ªêNG AI LOCAL - KH√îNG C·∫¶N OPENAI")
        logger.info("="*80)
        
        # 1. T·∫°o n·ªôi dung m·∫´u
        content = create_sample_content()
        logger.info(f"‚úì T·∫°o n·ªôi dung m·∫´u")
        
        # 2. Chunking
        logger.info("\nüì¶ Chunking...")
        chunks = chunk_text(content, chunk_size=500)
        logger.info(f"‚úì T·∫°o {len(chunks)} chunks")
        
        # 3. Build RAG index
        logger.info("\nüîç Build RAG index v·ªõi embeddings local...")
        rag = LocalRAG()
        rag.build_index(chunks)
        
        # 4. Initialize Ollama LLM
        logger.info("\nü§ñ Initialize Ollama LLM...")
        llm = OllamaLLM(model="qwen2.5:3b")
        
        # 5. Config ƒë·ªÅ thi
        config = GlobalConfig(
            time_minutes=45,
            total_points=10.0,
            mcq_ratio=0.6,
            essay_ratio=0.4
        )
        
        # 6. Generate exam
        logger.info("\nüéì B·∫Øt ƒë·∫ßu sinh ƒë·ªÅ ki·ªÉm tra...")
        generate_exam_local(rag, llm, config)
        
        logger.info("\n‚úÖ DEMO HO√ÄN TH√ÄNH!")
        logger.info("üí° H·ªá th·ªëng ƒë√£ s·ª≠ d·ª•ng AI local ho√†n to√†n, kh√¥ng c·∫ßn OpenAI API!")
        
    except Exception as e:
        logger.error(f"‚ùå L·ªói: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
